{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Why are we building OLM v1?","text":"<p>Operator Lifecycle Manager's mission has been to manage the lifecycle of cluster extensions centrally and declaratively on Kubernetes clusters. Its purpose has always been to make installing, running, and updating functional extensions to the cluster easy, safe, and reproducible for cluster administrators and PaaS administrators, throughout the lifecycle of the underlying cluster. </p> <p>OLM v0 was focused on providing unique support for these specific needs for a particular type of cluster extension, which have been coined as operators.  Operators are classified as one or more Kubernetes controllers, shipping with one or more API extensions (CustomResourceDefinitions) to provide additional functionality to the cluster. After running OLM v0 in production clusters for a number of years, it became apparent that there's an appetite to deviate from this coupling of CRDs and controllers, to encompass the lifecycling of extensions that are not just operators.</p> <p>OLM has been helping to define lifecycles for these extensions in which the extensions * get installed, potentially causing other extensions to be installed as well as dependencies  * get customized with the help of customizable configuration at runtime  * get upgraded to newer version/s following upgrade paths defined by extension developers  * and finally, get decommissioned and removed. In the dependency model, extensions can rely on each other for required services that are out of scope of the primary purpose of an extension, allowing each extension to focus on a specific purpose. </p> <p>OLM also prevents conflicting extensions from running on the cluster, either with conflicting dependency constraints or conflicts in ownership of services provided via APIs. Since cluster extensions need to be supported with an enterprise-grade product lifecycle, there has been a growing need for allowing operator authors to limit installation and upgrade of their extension by specifing addtional environmental constraints as dependencies, primarily to align with what was tested by the operator author's QE processes. In other words, there is an evergrowing ask for OLM to allow the author to enforce these support limitations in the form of additional constraints specified by operator authors in their packaging for OLM.</p> <p>During their lifecycle on the cluster, OLM also manages the permissions and capabilities extensions have on the cluster as well as the permission and access tenants on the cluster have to the extensions. This is done using the Kubernetes RBAC system, in combination with tenant isolation using Kubernetes namespaces. While the interaction surface of the extensions is solely composed of Kubernetes APIs the extensions define, there is an acute need to rethink the way tenant(i.e consumers of extensions) isolation is achieved. The ask from OLM, is to provide tenant isolation in a more intuitive way than is implemented in OLM v0</p> <p>OLM also defines a packaging model in which catalogs of extensions, usually containing the entire version history of each extension, are made available to clusters for cluster users to browse and select from. While these catalogs have so far been packaged and shipped as container images, there is a growing appetite to allow more ways of packaging and shipping these catalogs, besides also simplifying the building process of these catalogs, which so far have been very costly. The effort to bring down the cost was kicked off in OLM v0 with conversion of the underlying datastore for catalog metadata to File-based Catalogs, with more effort being invested to slim down the process in v1. Via new versions of extensions delivered with this packaging system, OLM is able to apply updates to existing running extensions on the cluster in a way where the integrity of the cluster is maintained and constraints and dependencies are kept satisfied.</p> <p>The scope of OLM's area of operation in v0 is the one cluster it is running on, with namespace-based handling of catalog access and extension API accessibility and discoverability. Expansion of this scope is indirectly expected through the work of the Kubernetes Control Plane (kcp) project, which in its first incarnation will likely use its own synchronization mechanism to get OLM-managed extensions deployed eventually on one or more physical clusters from a shared, virtual control plane called a \u201cworkspace\u201d. While this is an area under active development and subject to change, OLM will most likely need to become aware of kcp in a future state. In v1 of OLM, the scope of OLM will increase to span multiple clusters following the kcp model, though likely many aspects of this will become transparent to OLM itself through the workspace abstraction that kcp provides. So in other words, what needs to change in OLM 1.0 is how all of the tasks mentioned above are carried out from the user perspective, and how much control users have in the process, and which persona is involved.</p> <p>For a detailed writeup of OLM v1 requirements, please read the Product Requiment Documentation</p>"},{"location":"#the-olm-community","title":"The OLM community","text":"<p>The OLM v1 project is being tracked in a Github project</p> <p>You can reach out to the OLM community for feedbacks/discussions/contributions in the following channels:  * Kubernetes slack channel: #olm-dev * Operator Framework on Google Groups</p>"},{"location":"components/","title":"Components","text":"<p>OLM v1 is composed of various component projects: </p> <ul> <li> <p>operator-controller: Operator-controller is the central component of OLM v1, that consumes all of the components below to extend Kubernetes to allows users to install, and manage the lifecycle of other extensions</p> </li> <li> <p>rukpak: RukPak is a pluggable solution for the packaging and distribution of cloud-native content and supports advanced strategies for installation, updates, and policy. The project provides a content ecosystem for installing a variety of artifacts, such as Git repositories, Helm charts, OLM bundles, and more onto a Kubernetes cluster. These artifacts can then be managed, scaled, and upgraded in a safe way to enable powerful cluster extensions. At its core, RukPak is a small set of APIs, packaged as Kubernetes CustomResourceDefinitions, and controllers that watch for those APIs. These APIs express what content is being installed on-cluster and how to create a running deployment of the content.</p> </li> <li> <p>deppy: Deppy is a Kubernetes API that runs on- or off-cluster for resolving constraints over catalogs of RukPak bundles. Deppy is part of the next iteration of OLM and was first introduced here. The initial goal of the project is to remove the dependency manager from the Operator Lifecycle Manager (OLM) and make it its own generic component.</p> </li> <li> <p>catalogD: Catalogd is a Kubernetes extension that unpacks file-based catalog (FBC) content that is packaged and shipped in container images, for consumption by clients on-clusters (unpacking from other sources, like git repos, OCI artifacts etc, are in the roadmap for catalogD). As component of the Operator Lifecycle Manager (OLM) v1 microservices architecture, catalogD hosts metadata for Kubernetes extensions packaged by the authors of the extensions, as a result helping customers discover installable content.</p> </li> </ul>"},{"location":"olmv1_roadmap/","title":"Olmv1 roadmap","text":"<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p>"},{"location":"olmv1_roadmap/#olm-v1-roadmap","title":"OLM v1 roadmap","text":""},{"location":"olmv1_roadmap/#olm-overview-and-use-cases","title":"OLM overview and use cases","text":"<p>OLMs purpose is to manage cluster extensions centrally and declaratively on Kubernetes clusters. Its mission is to make installing, running, and updating functional add-ons to the cluster easy, safe, and reproducible for cluster administrators and PaaS administrators throughout the lifecycle of the underlying cluster as well as those of add-ons. The scope of OLM is currently the one cluster it is running on.</p> <p>OLM has unique support for the specific needs of cluster extensions, which are commonly referred to as operators. These are classified as one or more Kubernetes controllers shipping with one or more API extensions (CustomResourceDefinitions) to provide additional functionality to the cluster (though there are deviations from this coupling of CRDs and controllers, discussed below). They are managed centrally by OLM running on the cluster, where OLMs functionality is implemented following the Kubernetes operator pattern as well.</p> <p>OLM defines a lifecycle for these extensions in which they get installed, potentially causing other extensions to be installed as well, a limited set of customization of configuration at runtime, an update model following a path defined by the extension developer, and eventual decommission and removal.</p> <p>There is a dependency model in which extensions can rely on each other for required services (such as Apache Kafka) in a specific version when needed that are out of scope of the primary purpose of the extension, allowing each extension to focus on a specific purpose. OLM also prevents conflicting extensions from running on the cluster, either with conflicting dependency constraints or conflicts in ownership of services provided via APIs.</p> <p>Since cluster extensions need to be supported with an enterprise-grade product lifecycle the specific scenarios in which an extension can be installed or updated are going to be limited by the author of the extension, primarily to align with what was tested by their QE processes. OLM allows the author to enforce these support  limitations in the form of additional constraints.</p> <p>During their lifecycle on the cluster, OLM also manages the permissions and capabilities extensions have on the cluster as well as the permission and access tenants on the cluster have to the extensions. This is done using the Kubernetes RBAC system in combination with tenant isolation using Kubernetes namespaces. The interaction surface of the extensions is solely composed of Kubernetes APIs the extensions define.</p> <p>OLM also defines a packaging model in which catalogs of extensions, usually containing the entire version history of each extension, are made available to clusters for cluster users to browse and select from. As extensions can only be installed once in a cluster as cluster-wide singletons, it is likely for an extension to oversee multiple versions of service and utilize a distinct versioning scheme. To enable cluster users to make informed decisions about which extension version to install or update, a distinct field serves the purpose of specifying a list of service versions (e.g. Apache Kafka) managed by the extension. This field accommodates extensions that provide multiple services (e.g. both Quay and Clair) as well, ensuring comprehensive coverage of all provided services. Via new versions of extensions delivered with this packaging system, OLM is able to apply updates to existing running extensions on the cluster in a way where the integrity of the cluster is maintained and constraints and dependencies are kept satisfied.</p> <p>The scope of all these features is a single cluster so far with namespace-based handling of catalog access and extension API accessibility and discoverability. Expansion of this scope is indirectly expected through the work of the Kubernetes Control Plane (kcp) project, which in its first incarnation will likely use its own synchronization mechanism to get OLM-managed extensions deployed eventually on one or more physical clusters from a shared, virtual control plane called a \u201cworkspace\u201d. This is an area under active development and subject to change, OLM might need to become aware of kcp in a future state.</p>"},{"location":"olmv1_roadmap/#verdict-the-purpose-of-olm-remains-the-same-the-scope-of-olm-will-increase-to-span-multiple-clusters-following-the-kcp-model-though-likely-many-aspects-of-this-will-become-transparent-to-olm-itself-through-the-workspace-abstraction-that-kcp-provides-what-needs-to-change-in-olm-10-is-how-the-above-tasks-are-carried-out-from-the-user-perspective-and-how-much-control-users-have-in-the-process-and-which-persona-is-involved","title":"Verdict: The purpose of OLM remains the same. The scope of OLM will increase to span multiple clusters following the kcp model, though likely many aspects of this will become transparent to OLM itself through the workspace abstraction that kcp provides. What needs to change in OLM 1.0 is how the above tasks are carried out from the user perspective and how much control users have in the process and which persona is involved.","text":"<p>title: Product Requriement Doc layout: default nav_order: 2</p> <p>upstream/main</p>"},{"location":"olmv1_roadmap/#olm-v1-roadmap_1","title":"OLM v1 roadmap","text":""},{"location":"olmv1_roadmap/#functional-requirements","title":"Functional Requirements","text":"<p>Priority Rating: 1 highest, 2 medium, 3 lower (e.g. P2 = Medium Priority)</p>"},{"location":"olmv1_roadmap/#f1-extension-catalogs-p1","title":"F1 - Extension catalogs (P1)","text":"<p>The existing OLM concepts around catalogs, packages and channels is to be used as a basis for below functional requirements.</p>"},{"location":"olmv1_roadmap/#f2-extension-catalog-discovery-p2","title":"F2 - Extension catalog discovery (P2)","text":"<p>Unprivileged tenants need to have the ability to discover extensions that are available to install. In particular users need to be able to discover all versions in all channels that an extension package defines in a catalog. The privilege should be given at the discretion of the cluster administrator.</p>"},{"location":"olmv1_roadmap/#f3-dependency-preview-p3","title":"F3 - Dependency preview (P3)","text":"<p>Before extension installation, OLM needs to introspect the dependencies of an extension and present a preview of the resolution result to the user to let the user confirm they are ok with this.</p>"},{"location":"olmv1_roadmap/#f4-dependency-review-p3","title":"F4 - Dependency review (P3)","text":"<p>For installed extensions OLM needs to surface the dependency relationship to other installed extensions and also highlight which other extensions depend on a particular extension so users are informed about the relationships at runtime.</p>"},{"location":"olmv1_roadmap/#f5-permission-preview-p2","title":"F5 - Permission preview (P2)","text":"<p>Before extension installation and updates, OLM needs to allow introspection of the permissions the extension requires on cluster and dependencies APIs. This is especially important during updates when the permission scope increases in which case updates should be blocked until approved (F14).</p>"},{"location":"olmv1_roadmap/#f6-installupdate-preflight-checks-p1","title":"F6 - Install/Update preflight checks (P1)","text":"<p>When installing and updating extensions OLM should carry out a set of preflight checks to prevent installations from going into a failed state as a result of attempting an upgrade or install. Preflight checks should include availability and (if applicable) health of (running) dependencies and any cluster runtime constraints (F19).</p>"},{"location":"olmv1_roadmap/#f7-extension-installation-p1","title":"F7 - Extension installation (P1)","text":"<p>OLM needs a declarative way to install extensions either from a catalog of extensions or directly from an OCI image. Should the installation attempt fail due to unfilled requirements, constraints, preflight checks or dependencies there needs to be an option to force the install. Extensions are cluster-wide singletons, thus they can only be installed once in the cluster and are managed at cluster-scope.</p>"},{"location":"olmv1_roadmap/#f8-semver-based-update-policy-p2","title":"F8 - Semver-based update policy (P2)","text":"<p>OLM should allow users to specify if and when extensions are updated. Manual update policy should include the user explicitly approving an update to be installed. An automatic update policy should allow updates to automatically be applied as soon as they are available and should provide further conditions upon which an update is to be applied. Conditions concern version changes of the extension, specifically: automatic updates on z-streams only, automatic updates on y-streams only, always automatic update. Updates across channels are outside of the update policy.</p>"},{"location":"olmv1_roadmap/#f9-update-notification-p3","title":"F9 - Update notification (P3)","text":"<p>As updates can be made available at any time using OLMs existing over-the-air catalog update capabilities, OLM should provide events / notifications on the platform to notify users about available but not yet approved updates of existing installed extensions, specifically so that graphical consoles can pick them up and visualize them. Automatically applied updates as per F8 should also create notifications.</p>"},{"location":"olmv1_roadmap/#f10-extension-updates-p2","title":"F10 - Extension updates (P2)","text":"<p>As extensions get updated, either automatically or manually, OLM replaces the older version of the extensions with a newer version atomically. Up until any custom code or conversion logic runs, an update should be able to be rolled back (F23). When multiple extensions are updated to satisfy an update request, the update policy of each extension needs to be respected to allow users to pin certain extensions to installed versions or certain types of updates (e.g. z-stream only). It should also be possible to force an update to a certain version, even if there is no direct path as per the graph metadata. Otherwise all versions along the update path should be allowed for selecting by the user as the desired target version.Otherwise all versions on</p>"},{"location":"olmv1_roadmap/#f11-request-approval-flow-for-installs-updates-p2","title":"F11 - Request / Approval Flow for Installs / Updates (P2)","text":"<p>To support multi-tenant environments a request / approval flow is desirable for generally available content within default catalogs. In this model any tenant with enough privilege can discover installable content and can trigger an install request, which can in turn be approved or denied by a more privileged administrative role. Such requests should also have timeouts. Administrators should have the ability to define a list of extensions that are automatically approved at the scope of a namespace. Administrators should be able to get aware of unapproved requested via alerts triggered by the platform.</p>"},{"location":"olmv1_roadmap/#f12-installed-extension-discovery-p1","title":"F12 - Installed extension discovery (P1)","text":"<p>Unprivileged tenants need to be able to discover installed extensions if they are offering services for them to use in their namespace. OLM needs to provide distinct controls for installed operators which administrators can use to regulate in which namespaces extensions are discoverable by tenants, irrespective of the namespaces in which the operator has permissions on cluster APIs (see F13)</p>"},{"location":"olmv1_roadmap/#f13-extension-permissions-management-p1","title":"F13 - Extension permissions management (P1)","text":"<p>Administrative personas need to be able to configure in which namespaces in the cluster the extension can get the requested permissions the extension author deems required. The control needs to be independent of the controls in F12. Extensions should always be given permissions to update their own APIs (if they define any) to inform users about potential lack of permissions in their namespace.</p>"},{"location":"olmv1_roadmap/#f14-extension-permissions-escalation-checks-p2","title":"F14 - Extension permissions escalation checks (P2)","text":"<p>If, in the course of an update, an extension requests more permissions than the currently installed version, an automatic update is blocked and an administrative persona needs to specifically approve the update by default. The user who installed the extension can opt out of these permission increase checks for the purpose of automation.</p>"},{"location":"olmv1_roadmap/#f15-selective-extension-permissions-grants-p3","title":"F15 - Selective extension permissions grants (P3)","text":"<p>Administrative personas can choose to give extensions only a subset of the permissions it requests. This should be manageable at a per namespace level.</p>"},{"location":"olmv1_roadmap/#f16-extension-removal-p1","title":"F16 - Extension removal (P1)","text":"<p>Administrative personas need to be able to remove an extension including all the content that was part of the original installation bundle. Special handling should be implemented for CRDs, which when not removed, are left behind in a functioning state (i.e.any dependencies on running controllers like conversion webhooks need to be removed). When they are to be removed this can only happen if the user opts into F17. Special care also needs to be taken to allow the extension to perform any clean upon getting a signal to be removed. Components need to be removed in an order that allows the extension to handle a graceful shutdown.</p>"},{"location":"olmv1_roadmap/#f17-extension-cascading-removal-p2","title":"F17 - Extension cascading removal (P2)","text":"<p>OLM needs to be able to cleanly remove an extension entirely, which means deleting CRDs and other resources on the cluster. In particular this means the removal of all custom resource instances of the CRDs to be deleted and all extensions with a hard dependency. A user needs to actively opt-in to this behavior and OLM has to sequence the successful removal of all affected components and the extension itself.</p>"},{"location":"olmv1_roadmap/#f18-standalone-extension-bundle-installation-p2","title":"F18 - Standalone extension bundle installation (P2)","text":"<p>For local development OLM should allow the installation of an extension by directly referring to the bundle / OCI image in a container registry rather than a package name of an operator in a catalog containing the image in order to simplify testing and delivering hotfixes.</p>"},{"location":"olmv1_roadmap/#f19-extension-constraints-p1","title":"F19 - Extension constraints (P1)","text":"<p>OLM needs to allow extensions to specify constraints towards aspects of the running cluster and other currently installed or future extensions. Aspects of the running cluster should include software version, resource utilization, overall resource availability and state of configuration settings. These constraints need to be evaluated when extensions are attempted to be installed or updated.</p>"},{"location":"olmv1_roadmap/#f20-extension-health-p1","title":"F20 - Extension health (P1)","text":"<p>OLM needs to be able to report the overall health state of the extension on a cluster along the following set of aspects: presence of all required objects from the extension bundle, health of all components that have a liveness / readiness endpoint, presence and health of all other extensions the extension in question has a dependency on as well as evaluation of all additional constraints from F19. An extension that was forced to install despite missing / unhealthy dependencies and violated constraints has a reduced health scope down to the liveness / readiness endpoint.</p>"},{"location":"olmv1_roadmap/#f21-custom-extension-health-p3","title":"F21 - Custom extension health (P3)","text":"<p>OLM should provide a way for extensions to report an aggregate health state with custom logic. This should align with other communications channels that are also used for extensions to declare update readiness (F25). This way extensions can report health more accurately than what OLM reports today based on simple readiness of the extension controller pod. Clients like graphical consoles should be able to make use of this to supply additional overall health states of extensions that provide some form of control plane by the user of other extensions.</p>"},{"location":"olmv1_roadmap/#f22-best-effort-resolution-p2","title":"F22 - Best effort resolution (P2)","text":"<p>OLM should always try its best to resolve installation and update requests with the currently available and healthy set catalogs to resolve against. Intermittently or permanently failed catalogs should not block resolution for installation and updates. Fulfilling user requests is valued higher than determinism of results.</p>"},{"location":"olmv1_roadmap/#f23-opt-in-to-fallback-rollback-p2","title":"F23 - Opt-in to fallback / rollback (P2)","text":"<p>OLM should allow operator developers to specify whether or not it is safe to rollback from a particular current version of the operator to an author-specified previous version, once an extension update has passed pre-flights checks in F10 but subsequently failed to become available or carry out a migration. In these cases OLM should allow the administrator to downgrade the operator to the specific previous version. OLM should also respect this downgrade path when conducting updates that fail and use it to fail back to the previous version of the operator indicating that the downgrade path is supported. Extension uptime is an important goal of OLM.</p>"},{"location":"olmv1_roadmap/#f24-extension-overrides-p2","title":"F24 - Extension Overrides (P2)","text":"<p>Components deployed as part of extensions will require user-provided modifications at runtime in order to aid features like placement, networking configuration, proxy configuration etc. that require customization of volume mounts, annotations, labels, environment variables, taints, tolerations, node selectors, affinity, and resources. OLM should support accepting these customizations to the extension as input from the user prior or after the installation of an extension and apply them to applicable objects such as Deployments, StatefulSets, ReplicatSets.</p>"},{"location":"olmv1_roadmap/#f25-extension-controlled-update-readiness-p2","title":"F25 - Extension-controlled Update Readiness (P2)","text":"<p>Extensions should be able to control their readiness for updates. An extension could be on a critical path or in a state where updates would lead to disruption or worst-case: outages. OLM should respect these update readiness signals and allow the extension to signal readiness differentiated to what nature the update is based on semantic versioning rules, i.e. patch updates vs. minor or major updates. Once the signal is encountered, OLM should block the update until the signal disappears.</p>"},{"location":"olmv1_roadmap/#f26-canary-style-rollouts-p3","title":"F26 - Canary Style Rollouts (P3)","text":"<p>OLM should have an opinion about how administrators can carry out roll outs of new extension versions that coexist with already installed versions of the extension, in particular if the extension only ships a controller. While conflicting CRDs cannot co-exist, controllers that only selectively reconcile objects (Ingress operator pattern) can. OLM should support these deployment styles while ensuring integrity of cluster-level extensions like CRDs.</p>"},{"location":"olmv1_roadmap/#f27-pluggable-certificate-management-p2","title":"F27 - Pluggable certificate management (P2)","text":"<p>OLM should rely on other controllers to create and lifecycle TLS certificates required to drive the functionality of certain extensions, like webhooks that need to trust /  need to be trusted by the cluster's API server. OLM should not implement any certificate handling itself. In a first implementation support should be established for cert-manager.</p> <p>F28 - Provided service versions (P2): Extensions can offer multiple services with specific version ranges. To help cluster users make informed decisions about which extension version to install or update, OLM should provide a separate field for extension developers to specify the provided services and their respective versions. This will allow for declaring extension dependencies as an additonal constraint on specific service versions and enable flexibility in releasing extensions and their corresponding services at different cadences. In disconnected environments, cluster users should be able to deselect provided services and versions during image mirroring.</p>"},{"location":"olmv1_roadmap/#behavioral-requirements","title":"Behavioral Requirements","text":"<p>Priority Rating: 1 highest, 2 medium, 3 lower (ex. P2 = Medium Priority)</p>"},{"location":"olmv1_roadmap/#b1-single-api-control-surface-p1","title":"B1 - Single API control surface (P1)","text":"<p>While the underlying implementation of the functional requirements can be carried out by different controllers with different APIs, to the administrative and non-administrative users there should be a single, cluster-level API that represents an installed extension with all high level controls described in / required by F4, F7, F8, F10, F13, F14, F15, F16, F17, F18, F21, F22, F23 and F24.</p>"},{"location":"olmv1_roadmap/#b2-gitops-friendly-api-surface-p1","title":"B2 - GitOps-friendly API surface (P1)","text":"<p>In many cases OLM APIs will not be used by a human via a CLI or GUI interactively but declaratively through a GitOps pipeline. This means the primary OLM API to lifecycle an extension cannot leak abstractions to other APIs for initial deployment or reconfiguration. Modifications must not require conditional lookup or modifications of other objects on the cluster that are created as part of the declarative intent stored in git in the form of YAML manifest files.</p>"},{"location":"olmv1_roadmap/#b3-declarative-api-p1","title":"B3 - Declarative API (P1)","text":"<p>As an operator itself, OLMs API controls have to allow for operations to be carried out solely declaratively. This mandates continuous reconciliation and eventual consistency of desired state. OLM should not conduct one-off operations. OLM should not require either clean up of failed operations or restating intent to retry a failed operation (with the exception of F11).</p>"},{"location":"olmv1_roadmap/#b4-event-trail-p2","title":"B4 - Event trail (P2)","text":"<p>OLM should make heavy use of Kubernetes events to leave an audit trail of tasks and actions carried out on the cluster. For expected failure scenarios administrators should not need to consult the OLM controller logs for debugging but solely rely on events and status conditions (see also B6).</p>"},{"location":"olmv1_roadmap/#b5-force-overrides-p1","title":"B5 - Force overrides (P1)","text":"<p>While OLM has a lot of opinions about safe operations with cluster extensions they do not apply all the time since OLM cannot possibly foresee how extensions behave at runtime. It needs to yield to the user in cases where they have more certainty about what's going to happen based on their background knowledge of the cluster or the operator. It should offer ways to force-override decisions that OLM made that block the user from proceeding in a certain direction, especially in the areas of extension installation, removal and updates.</p>"},{"location":"olmv1_roadmap/#b6-human-readable-status-extensions-information-p2","title":"B6 - Human-readable status extensions information (P2)","text":"<p>Whenever OLM is in the process of or having reached or failed to reach a desired state it needs to update the user about what is happening / what has happened without assuming knowledge about OLM internal or implementation details.</p>"},{"location":"olmv1_roadmap/#b7-scalability-resource-consumption-p1","title":"B7 - Scalability &amp; Resource consumption (P1)","text":"<p>OLM is used on clusters with hundreds to thousands of namespaces and tenants. Its API controls, specifically for F2 and F12 need to be built in such a way that resource consumption scales linearly with usage and cluster size and the overall resource usage envelope stays within manageable bounds that does not put the cluster stability, especially that of the API server at risk. System memory especially is a scarce resource.</p>"},{"location":"olmv1_roadmap/#compatibility-requirements","title":"Compatibility Requirements:","text":""},{"location":"olmv1_roadmap/#c1-compatibility-with-existing-extensions-p1","title":"C1 - Compatibility with existing extensions  (P1)","text":"<p>OLM should be able to manage extensions packaged with the current bundle format in the way described by the functional and behavior requirements when the bundle supports AllNamespace installation mode.</p>"},{"location":"olmv1_roadmap/#c2-compatibility-with-existing-catalogs-p1","title":"C2 - Compatibility with existing catalogs (P1)","text":"<p>OLM should be able to retrieve and update extensions that adhere to C1 from the currently supported catalog formats (File-based catalogs).</p>"},{"location":"olmv1_roadmap/#c3-incompatibility-with-existing-extensions-p1","title":"C3 - Incompatibility with existing extensions (P1)","text":"<p>OLM 1.0 does not support managing bundles or extension versions that do not support AllNamespace installation mode with the new set of APIs or flows</p>"},{"location":"olmv1_roadmap/#assumptions","title":"Assumptions","text":"<ul> <li> <p>No additional tenancy model will be introduced at the control plane / API layer of Kubernetes upstream</p> </li> <li> <p>kcp doesn\u2019t fundamentally change OLMs role and responsibilities around managing extensions (at least initially)</p> </li> <li> <p>OLM will move to a descoped, cluster-wide singleton model for cluster extensions, extension management isn\u2019t namespaced</p> </li> </ul>"},{"location":"olmv1_roadmap/#constraints","title":"Constraints","text":"<ul> <li>Only operator bundles with \u201cAllNamespace\u201d mode installation support can be lifecycled with the new APIs / flows in OLM 1.0</li> </ul>"},{"location":"olmv1_roadmap/#dependencies","title":"Dependencies","text":"<ul> <li>\"F13 - Extension permissions management (P1)\" and \"F12 - Installed extension discovery (P1)\" will land prior to the GA of OLM 1.0 to unblock most operators that do not support AllNamespace installation mode today.</li> </ul>"},{"location":"olmv1_roadmap/#migration","title":"Migration","text":"<ul> <li> <p>A new set of APIs is introduced in parallel to the existing set of APIs</p> </li> <li> <p>Users opt-in to the new set of APIs, potentially resulting in a reinstall of their extension if required</p> </li> <li> <p>Extensions that are shipped with the current bundle format with AllNamespace mode can simply be reused with the new set of APIs and controls</p> </li> <li> <p>Extensions that do not support AllNamespace mode cannot be managed with the new APIs</p> </li> <li> <p>Migration scripting is provided to mass-convert existing installed extensions (\u201cSubscription\u201d / \u201cOperatorGroup\u201d objects) on existing clusters to the new OLM 1.0 model assuming they are compatible</p> </li> <li> <p>Operator authors that are also SRE/Managed PaaS administrators are incentivized to make their operator compatible with the requirements of OLM 1.0 to reap the operational benefits</p> </li> </ul>"},{"location":"olmv1_roadmap/#todo","title":"TODO","text":"<ul> <li>Definition of \"extension\"</li> <li>Does OLM become ELM?  Does this provide of provisioning bundles that do not add APIs?</li> </ul>"},{"location":"Tasks/adding-a-catalog/","title":"Adding a catalog of operators to the cluster","text":"<p>Operator authors have the mechanisms to offer their product as part of a curated catalog of operators, that they can push updates to over-the-air (eg publish new versions, publish patched versions with CVEs, etc). Cluster admins can sign up to receive these updates on clusters, by adding the catalog to the cluster. When a catalog is added to a cluster, the kubernetes extension packages (operators, or any other extension package) in that catalog become available on cluster for installation and receiving updates.  </p> <p>For example, the k8s-operatorhub/community-operators is a catalog of curated operators that contains a list of operators being developed by the community. The list of operators can be viewed in Operatorhub.io. This catalog is distributed as an image quay.io/operatorhubio/catalog for consumption on clusters. </p> <p>To consume this catalog on cluster, create a <code>Catalog</code> Custom Resource(CR) with the image specified in the <code>spec.source.image</code> field: </p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: catalogd.operatorframework.io/v1alpha1\nkind: Catalog\nmetadata:\n  name: operatorhubio\nspec:\n  source:\n    type: image\n    image:\n      ref: quay.io/operatorhubio/catalog:latest\nEOF\n</code></pre> <p>The packages made available for installation/receiving updates on cluster can then be explored by querying the <code>Package</code> and <code>BundleMetadata</code> CRs: </p> <pre><code>$ kubectl get packages \nNAME                                                     AGE\noperatorhubio-ack-acm-controller                         3m12s\noperatorhubio-ack-apigatewayv2-controller                3m12s\noperatorhubio-ack-applicationautoscaling-controller      3m12s\noperatorhubio-ack-cloudtrail-controller                  3m12s\noperatorhubio-ack-dynamodb-controller                    3m12s\noperatorhubio-ack-ec2-controller                         3m12s\noperatorhubio-ack-ecr-controller                         3m12s\noperatorhubio-ack-eks-controller                         3m12s\noperatorhubio-ack-elasticache-controller                 3m12s\noperatorhubio-ack-emrcontainers-controller               3m12s\noperatorhubio-ack-eventbridge-controller                 3m12s\noperatorhubio-ack-iam-controller                         3m12s\noperatorhubio-ack-kinesis-controller                     3m12s\noperatorhubio-ack-kms-controller                         3m12s\noperatorhubio-ack-lambda-controller                      3m12s\noperatorhubio-ack-memorydb-controller                    3m12s\noperatorhubio-ack-mq-controller                          3m12s\noperatorhubio-ack-opensearchservice-controller           3m12s\n.\n.\n.\n\n$ kubectl get bundlemetadata \nNAME                                                            AGE\noperatorhubio-ack-acm-controller.v0.0.1                         3m58s\noperatorhubio-ack-acm-controller.v0.0.2                         3m58s\noperatorhubio-ack-acm-controller.v0.0.4                         3m58s\noperatorhubio-ack-acm-controller.v0.0.5                         3m58s\noperatorhubio-ack-acm-controller.v0.0.6                         3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.10               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.11               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.12               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.13               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.14               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.15               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.16               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.17               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.18               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.19               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.20               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.21               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.22               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.9                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.0                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.1                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.2                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.3                3m58s\n.\n.\n.\n</code></pre>"},{"location":"Tasks/deleting-an-operator/","title":"Deleting an operator from the cluster","text":"<p>Deleting an operator is as simple as deleting an existing Operator CR: </p> <pre><code>$ kubectl get operators \nNAME                          AGE\noperatorhubio-argocd-operator   53s\n\n$ kubectl delete operator argocd-operator \noperator.operators.operatorframework.io \"argocd-operator\" deleted\n$ kubectl get namespaces | grep argocd\n$\n$ kubectl get crds | grep argocd-operator \n$\n</code></pre>"},{"location":"Tasks/explore-available-packages/","title":"Exploring packages available for installation on cluster","text":"<p>The packages available for installation/receiving updates on cluster can be explored by querying the <code>Package</code> and <code>BundleMetadata</code> CRs: </p> <pre><code>$ kubectl get packages \nNAME                                                     AGE\noperatorhubio-ack-acm-controller                         3m12s\noperatorhubio-ack-apigatewayv2-controller                3m12s\noperatorhubio-ack-applicationautoscaling-controller      3m12s\noperatorhubio-ack-cloudtrail-controller                  3m12s\noperatorhubio-ack-dynamodb-controller                    3m12s\noperatorhubio-ack-ec2-controller                         3m12s\noperatorhubio-ack-ecr-controller                         3m12s\noperatorhubio-ack-eks-controller                         3m12s\noperatorhubio-ack-elasticache-controller                 3m12s\noperatorhubio-ack-emrcontainers-controller               3m12s\noperatorhubio-ack-eventbridge-controller                 3m12s\noperatorhubio-ack-iam-controller                         3m12s\noperatorhubio-ack-kinesis-controller                     3m12s\noperatorhubio-ack-kms-controller                         3m12s\noperatorhubio-ack-lambda-controller                      3m12s\noperatorhubio-ack-memorydb-controller                    3m12s\noperatorhubio-ack-mq-controller                          3m12s\noperatorhubio-ack-opensearchservice-controller           3m12s\n.\n.\n.\n\n$ kubectl get bundlemetadata \nNAME                                                            AGE\noperatorhubio-ack-acm-controller.v0.0.1                         3m58s\noperatorhubio-ack-acm-controller.v0.0.2                         3m58s\noperatorhubio-ack-acm-controller.v0.0.4                         3m58s\noperatorhubio-ack-acm-controller.v0.0.5                         3m58s\noperatorhubio-ack-acm-controller.v0.0.6                         3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.10               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.11               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.12               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.13               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.14               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.15               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.16               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.17               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.18               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.19               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.20               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.21               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.22               3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.0.9                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.0                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.1                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.2                3m58s\noperatorhubio-ack-apigatewayv2-controller.v0.1.3                3m58s\n.\n.\n.\n</code></pre> <p>Individual <code>Package</code>/<code>BundleMetadata</code> CRs can then be explored more by retrieving their yamls. Eg the <code>operatorhubio-argocd-operator</code> CR has more detailed information about the <code>argocd-operator</code>: </p> <pre><code>$ kubectl get packages | grep argocd \noperatorhubio-argocd-operator                            5m19s\noperatorhubio-argocd-operator-helm                       5m19s\n\n$ kubectl get package operatorhubio-argocd-operator -o yaml \napiVersion: catalogd.operatorframework.io/v1alpha1\nkind: Package\nmetadata:\n  creationTimestamp: \"2023-06-16T14:34:04Z\"\n  generation: 1\n  labels:\n    catalog: operatorhubio\n  name: operatorhubio-argocd-operator\n  ownerReferences:\n  - apiVersion: catalogd.operatorframework.io/v1alpha1\n    blockOwnerDeletion: true\n    controller: true\n    kind: Catalog\n    name: operatorhubio\n    uid: 9a949664-9069-4376-9a66-a9921f7488e2\n  resourceVersion: \"3765\"\n  uid: 43396920-4af4-4daf-a069-be68b8a0631e\nspec:\n  catalog:\n    name: operatorhubio\n  channels:\n  - entries:\n    - name: argocd-operator.v0.0.11\n      replaces: argocd-operator.v0.0.9\n    - name: argocd-operator.v0.0.12\n      replaces: argocd-operator.v0.0.11\n    - name: argocd-operator.v0.0.13\n      replaces: argocd-operator.v0.0.12\n    - name: argocd-operator.v0.0.14\n      replaces: argocd-operator.v0.0.13\n    - name: argocd-operator.v0.0.15\n      replaces: argocd-operator.v0.0.14\n    - name: argocd-operator.v0.0.2\n    - name: argocd-operator.v0.0.3\n      replaces: argocd-operator.v0.0.2\n    - name: argocd-operator.v0.0.4\n      replaces: argocd-operator.v0.0.3\n    - name: argocd-operator.v0.0.5\n      replaces: argocd-operator.v0.0.4\n    - name: argocd-operator.v0.0.6\n      replaces: argocd-operator.v0.0.5\n    - name: argocd-operator.v0.0.8\n      replaces: argocd-operator.v0.0.6\n    - name: argocd-operator.v0.0.9\n      replaces: argocd-operator.v0.0.8\n    - name: argocd-operator.v0.1.0\n      replaces: argocd-operator.v0.0.15\n    - name: argocd-operator.v0.2.0\n      replaces: argocd-operator.v0.1.0\n    - name: argocd-operator.v0.2.1\n      replaces: argocd-operator.v0.2.0\n    - name: argocd-operator.v0.3.0\n      replaces: argocd-operator.v0.2.1\n    - name: argocd-operator.v0.4.0\n      replaces: argocd-operator.v0.3.0\n    - name: argocd-operator.v0.5.0\n      replaces: argocd-operator.v0.4.0\n    - name: argocd-operator.v0.6.0\n      replaces: argocd-operator.v0.5.0\n    name: \"\"\n  defaultChannel: \"\"\n  description: \"\"\n  icon:\n    data: PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+CjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2Z==\n    mediatype: image/svg+xml\n  packageName: argocd-operator\nstatus: {}\n</code></pre> <p>This CR is most helpful when exploring the versions of a package that are available for installation on cluster, and the upgrade graph of versions (eg if v0.5.0 of <code>argocd-operator</code> is installed on cluster, what is the next upgrade available? The answer is v0.6.0).</p>"},{"location":"Tasks/installing-an-operator/","title":"Installing an operator on cluster","text":"<p>Creating an Operator CR installs the operator on cluster:  </p> <pre><code>$ kubectl get packages | grep argocd \noperatorhubio-argocd-operator                            5m19s\noperatorhubio-argocd-operator-helm                       5m19s\n\n$ kubectl apply -f - &lt;&lt;EOF\napiVersion: operators.operatorframework.io/v1alpha1\nkind: Operator\nmetadata:\n  name: argocd-operator\nspec:\n  packageName: operatorhubio-argocd-operator\nEOF\n\n$ kubectl get operators \nNAME                          AGE\noperatorhubio-argocd-operator   53s\n\n$  kubectl get operator argocd-operator -o yaml \napiVersion: operators.operatorframework.io/v1alpha1\nkind: Operator\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"operators.operatorframework.io/v1alpha1\",\"kind\":\"Operator\",\"metadata\":{\"annotations\":{},\"name\":\"argocd-operator\"},\"spec\":{\"packageName\":\"argocd-operator\"}}\n  creationTimestamp: \"2023-06-21T14:57:50Z\"\n  generation: 1\n  name: argocd-operator\n  resourceVersion: \"10690\"\n  uid: 6e0c67a5-eb9c-41c6-a455-140b28714d34\nspec:\n  packageName: operatorhubio-argocd-operator\nstatus:\n  conditions:\n  - lastTransitionTime: \"2023-06-21T14:57:51Z\"\n    message: resolved to \"quay.io/operatorhubio/argocd-operator@sha256:1a9b3c8072f2d7f4d6528fa32905634d97b7b4c239ef9887e3fb821ff033fef6\"\n    observedGeneration: 1\n    reason: Success\n    status: \"True\"\n    type: Resolved\n  - lastTransitionTime: \"2023-06-21T14:57:57Z\"\n    message: installed from \"quay.io/operatorhubio/argocd-operator@sha256:1a9b3c8072f2d7f4d6528fa32905634d97b7b4c239ef9887e3fb821ff033fef6\"\n    observedGeneration: 1\n    reason: Success\n    status: \"True\"\n    type: Installed\n  installedBundleResource: quay.io/operatorhubio/argocd-operator@sha256:1a9b3c8072f2d7f4d6528fa32905634d97b7b4c239ef9887e3fb821ff033fef6\n  resolvedBundleResource: quay.io/operatorhubio/argocd-operator@sha256:1a9b3c8072f2d7f4d6528fa32905634d97b7b4c239ef9887e3fb821ff033fef6\n</code></pre> <p>The status condition type <code>Installed</code>:<code>true</code> indicates that the operator was installed successfully. We can confirm this by looking at the workloads that were created as a result of this operator installation: </p> <pre><code>$ kubectl get namespaces | grep argocd \nargocd-operator-system       Active   4m17s\n\n$ kubectl get pods -n argocd-operator-system \nNAME                                                 READY   STATUS    RESTARTS   AGE\nargocd-operator-controller-manager-bb496c545-ljbbr   2/2     Running   0          4m32s\n</code></pre>"}]}